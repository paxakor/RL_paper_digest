## Reinforcement Learning Testbed for Power-Consumption Optimization

1. Link: https://arxiv.org/abs/1808.10427
2. Code: https://github.com/IBM/rl-testbed-for-energyplus

-----

### Problem

Центры обработки данных потребляют 3% производимого в мире электричества. Большая чать этой энергии требуется
для охлаждения датацентра. В статье предлагается способ оптимизации потребления электроэнергии датацентром с
использованием обучения с подкреплением


Общепринятые подходы к управлению системой охлаждения центра обработки данных основаны на приближенных моделях
системы и окружающей среды, построенных на знаниях механического охлаждения и электрического и теплового управления.
Эти модели трудно конструировать и часто они приводят к субоптимальному или неустойчивому решению.


В работе показано, как можно использовать методы глубокого обучения с подкреплением для управления системой
охлаждения смоделированного центра обработки данных. В отличие от обычных алгоритмов управления, алгоритмы, основанные
на методах обучения с подкреплением, могут автоматически оптимизировать производительность системы без
необходимости использования явных знаний о модели. Необходимо разработать только систему вознаграждения.


Предложенный алгоритм оценили на платформе моделирования EnergyPlus с открытым исходным кодом.
Экспериментальные результаты показали, что можно достичь улучшения на 22% по сравнению с алгоритмом
управления на основе модели, встроенным в EnergyPlus.



### Solution

План:
1. Внедрить gym-like агента в симулятор.
2. Обучить модель.


#### EnergyPlusEnv

Для внедрения agent-based модуля принятия решений в систему авторам статьи пришлось модифицировать EnergyPlus.
Они добавили два метода: `ExtCtrlObs` и `ExtCtrlAct`, которые помогают передавать наблюдения (`s_i`) и действия (`a_i`)
между моделью и моделирующей системой.
Авторы статьи приводят детальное описание реализации взаимодействия модели с симулятором через высокоувовневую
систему управления EMS с использованием скриптов на EnergyPlus Runtime Language (Erl). Так же они замечают,
что их наработки можно использовать и для обучения под другие архитектуры датацентров.


#### TRPO-модель

В качестве DRL-based агента они использовали алгоритм Trust region policy optimization с перцептроном,
состоящим из двух полносвязных слоёв по 32 вершины с функцией активации `tanh`.
В качестве данных для обучения были взяты данные о погоде в пяти различных регионах.
Сравнивались 3 модели:
* Baseline: стандартный контроллер EnergyPlus.
* TRPO (CA): Алгоритм TRPO, обученный на данных о погоде в Сан-Франциско.
* TRPO (CA-CO-FL): Алгоритм TRPO, обученный на данных о погоде в городах Сан-Франциско, Годен и Тампа.

### Результаты

* Модель TRPO (CA) эффективна на локации CA, но даёт большой разброс на остальных локациях.
* Модель TRPO (CA-CO-FL) более эффективна на всех локациях. Позволяет экономить 22% электроэнергии,
расходуемого на охлаждение.

Температурные коридоры моделей в зависимости от локации:
<img src="https://i.postimg.cc/tCjgTYBQ/2019-03-18-144858-1138x698-scrot.png">

Продуктивность моделей в зависимости от локации:
<img src="https://i.postimg.cc/0Q7bWmVn/2019-03-18-145332-1489x380-scrot.png">
